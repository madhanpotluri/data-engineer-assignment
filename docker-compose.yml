services:
  # 1. PostgreSQL Database Service
  postgres:
    image: postgres:13
    container_name: postgres
    ports:
      - "5433:5432"
    env_file:
      - .env
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h postgres -p 5432"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 2. Apache Airflow Services
  airflow-webserver:
    build:
      context: . 
      dockerfile: ./docker/airflow/Dockerfile 
    image: airflow-local:latest 
    container_name: airflow-webserver
    restart: always
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    environment:
      # Use the correct postgres user and database
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://postgres:postgres123@postgres:5432/iot_data
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
    command: ["webserver"]

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./docker/airflow/Dockerfile
    image: airflow-local:latest
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    environment:
      # Use the correct postgres user and database
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://postgres:postgres123@postgres:5432/iot_data
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
    command: ["scheduler"]

  # 3. Apache Spark Cluster Services
  spark-master:
    build:
      context: .
      dockerfile: ./docker/spark-master/Dockerfile
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8081:8080" 

  spark-worker:
    build:
      context: .
      dockerfile: ./docker/spark-worker/Dockerfile
    container_name: spark-worker-1
    depends_on:
      - spark-master

volumes:
  postgres-data: {}

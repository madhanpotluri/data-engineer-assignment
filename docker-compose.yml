services:
  # 1. PostgreSQL Database Service
  postgres:
    image: postgres:13
    container_name: postgres
    ports:
      - "5433:5432"
    env_file:
      - .env
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h postgres -p 5432"]
      interval: 5s
      timeout: 5s
      retries: 5

  # 2. Apache Airflow Services
  airflow-webserver:
    build:
      context: . 
      dockerfile: ./docker/airflow/Dockerfile 
    image: airflow-local:latest 
    container_name: airflow-webserver
    restart: always
    ports:
      - "8080:8080"
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    environment:
      # Use the correct postgres user and database
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://postgres:postgres123@postgres:5432/iot_data
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
      - ./data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    command: ["webserver"]

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./docker/airflow/Dockerfile
    image: airflow-local:latest
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    environment:
      # Use the correct postgres user and database
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql://postgres:postgres123@postgres:5432/iot_data
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./logs:/opt/airflow/logs
      - ./data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    command: ["scheduler"]

  # 3. Apache Spark Cluster Services
  spark-master:
    build:
      context: .
      dockerfile: ./docker/spark-master/Dockerfile
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8081:8080" 

  spark-worker:
    build:
      context: .
      dockerfile: ./docker/spark-worker/Dockerfile
    container_name: spark-worker-1
    depends_on:
      - spark-master

  # 4. Data Visualization Dashboard
  dashboard:
    build:
      context: .
      dockerfile: ./docker/dashboard/Dockerfile
    container_name: iot-dashboard
    restart: always
    ports:
      - "8501:8501"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - DB_HOST=postgres
      - DB_PORT=5432
      - DB_NAME=iot_data
      - DB_USER=postgres
      - DB_PASSWORD=postgres123
    volumes:
      - ./data:/data
      - ./scripts:/scripts

  # 5. IoT Data Simulator (standalone)
  iot-simulator:
    build:
      context: .
      dockerfile: ./docker/spark-master/Dockerfile
    container_name: iot-simulator
    restart: "no"
    command: ["python3", "/scripts/iot_simulator.py", "--output-dir", "/data/landing_zone", "--interval", "5"]
    volumes:
      - ./data:/data
      - ./scripts:/scripts
    profiles:
      - simulation

volumes:
  postgres-data: {}
